{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b07f89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "ADDITIONAL_STOPWORDS = ['sc','ieee','cvpr','conference','nips','acm','ieee','kdd','1970', '1971', '1972', '1973', '1974', '1975', '1976', '1977', '1978', '1979', '1980', '1981', '1982', '1983', '1984', '1985', '1986', '1987', '1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021']\n",
    "science_stopwords = ['paper','papers','author','propose','result','results','show','experiments','present','novel','approach','include','included','proceeding','proceedings','contained','contains','high level','rights','right','reserved','ieee',\n",
    "                    'method','datset','corpus','copyright','sae','time','average','experimental','experiments','experiment','copyright','sae','datasets','performance','training','test','state','art','_','machine learning','deep learning','abstract','authors','topics','discussed','given','discussed','discuss','proof']\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d3db4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_stopwords = [\"a\",\"a's\",\"able\",\"about\",\"above\",\"according\",\"accordingly\",\"across\",\"actually\",\"after\",\"afterwards\",\"again\",\"against\",\"ain't\",\"all\",\"allow\",\"allows\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"am\",\"among\",\"amongst\",\"an\",\"and\",\"another\",\"any\",\"anybody\",\"anyhow\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"are\",\"aren't\",\"around\",\"as\",\"aside\",\"ask\",\"asking\",\"associated\",\"at\",\"available\",\"away\",\"awfully\",\"b\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"behind\",\"being\",\"believe\",\"below\",\"beside\",\"besides\",\"best\",\"better\",\"between\",\"beyond\",\"both\",\"brief\",\"but\",\"by\",\"c\",\"c'mon\",\"c's\",\"came\",\"can\",\"can't\",\"cannot\",\"cant\",\"cause\",\"causes\",\"certain\",\"certainly\",\"changes\",\"clearly\",\"co\",\"com\",\"come\",\"comes\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"contain\",\"containing\",\"contains\",\"corresponding\",\"could\",\"couldn't\",\"course\",\"currently\",\"d\",\"definitely\",\"described\",\"despite\",\"did\",\"didn't\",\"different\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"done\",\"down\",\"downwards\",\"during\",\"e\",\"each\",\"edu\",\"eg\",\"eight\",\"either\",\"else\",\"elsewhere\",\"enough\",\"entirely\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"exactly\",\"example\",\"except\",\"f\",\"far\",\"few\",\"fifth\",\"first\",\"five\",\"followed\",\"following\",\"follows\",\"for\",\"former\",\"formerly\",\"forth\",\"four\",\"from\",\"further\",\"furthermore\",\"g\",\"get\",\"gets\",\"getting\",\"given\",\"gives\",\"go\",\"goes\",\"going\",\"gone\",\"got\",\"gotten\",\"greetings\",\"h\",\"had\",\"hadn't\",\"happens\",\"hardly\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he's\",\"hello\",\"help\",\"hence\",\"her\",\"here\",\"here's\",\"hereafter\",\"hereby\",\"herein\",\"hereupon\",\"hers\",\"herself\",\"hi\",\"him\",\"himself\",\"his\",\"hither\",\"hopefully\",\"how\",\"howbeit\",\"however\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"ie\",\"if\",\"ignored\",\"immediate\",\"in\",\"inasmuch\",\"inc\",\"indeed\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"instead\",\"into\",\"inward\",\"is\",\"isn't\",\"it\",\"it'd\",\"it'll\",\"it's\",\"its\",\"itself\",\"j\",\"just\",\"k\",\"keep\",\"keeps\",\"kept\",\"know\",\"known\",\"knows\",\"l\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"let's\",\"like\",\"liked\",\"likely\",\"little\",\"look\",\"looking\",\"looks\",\"ltd\",\"m\",\"mainly\",\"many\",\"may\",\"maybe\",\"me\",\"mean\",\"meanwhile\",\"merely\",\"might\",\"more\",\"moreover\",\"most\",\"mostly\",\"much\",\"must\",\"my\",\"myself\",\"n\",\"name\",\"namely\",\"nd\",\"near\",\"nearly\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"no\",\"nobody\",\"non\",\"none\",\"noone\",\"nor\",\"normally\",\"not\",\"nothing\",\"novel\",\"now\",\"nowhere\",\"o\",\"obviously\",\"of\",\"off\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"on\",\"once\",\"one\",\"ones\",\"only\",\"onto\",\"or\",\"other\",\"others\",\"otherwise\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"outside\",\"over\",\"overall\",\"own\",\"p\",\"particular\",\"particularly\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"possible\",\"presumably\",\"probably\",\"provides\",\"q\",\"que\",\"quite\",\"qv\",\"r\",\"rather\",\"rd\",\"re\",\"really\",\"reasonably\",\"regarding\",\"regardless\",\"regards\",\"relatively\",\"respectively\",\"right\",\"s\",\"said\",\"same\",\"saw\",\"say\",\"saying\",\"says\",\"second\",\"secondly\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sensible\",\"sent\",\"serious\",\"seriously\",\"seven\",\"several\",\"shall\",\"she\",\"should\",\"shouldn't\",\"since\",\"six\",\"so\",\"some\",\"somebody\",\"somehow\",\"someone\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specified\",\"specify\",\"specifying\",\"still\",\"sub\",\"such\",\"sup\",\"sure\",\"t\",\"t's\",\"take\",\"taken\",\"tell\",\"tends\",\"th\",\"than\",\"thank\",\"thanks\",\"thanx\",\"that\",\"that's\",\"thats\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"there's\",\"thereafter\",\"thereby\",\"therefore\",\"therein\",\"theres\",\"thereupon\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"think\",\"third\",\"this\",\"thorough\",\"thoroughly\",\"those\",\"though\",\"three\",\"through\",\"throughout\",\"thru\",\"thus\",\"to\",\"together\",\"too\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"twice\",\"two\",\"u\",\"un\",\"under\",\"unfortunately\",\"unless\",\"unlikely\",\"until\",\"unto\",\"up\",\"upon\",\"us\",\"use\",\"used\",\"useful\",\"uses\",\"using\",\"usually\",\"uucp\",\"v\",\"value\",\"various\",\"very\",\"via\",\"viz\",\"vs\",\"w\",\"want\",\"wants\",\"was\",\"wasn't\",\"way\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"welcome\",\"well\",\"went\",\"were\",\"weren't\",\"what\",\"what's\",\"whatever\",\"when\",\"whence\",\"whenever\",\"where\",\"where's\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"whereupon\",\"wherever\",\"whether\",\"which\",\"while\",\"whither\",\"who\",\"who's\",\"whoever\",\"whole\",\"whom\",\"whose\",\"why\",\"will\",\"willing\",\"wish\",\"with\",\"within\",\"without\",\"won't\",\"wonder\",\"would\",\"would\",\"wouldn't\",\"x\",\"y\",\"yes\",\"yet\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"z\",\"zero\"]\n",
    "science_stopwords = science_stopwords+other_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0109d848",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "dlopen(/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scipy/spatial/_distance_wrap.cpython-310-darwin.so, 0x0002): symbol not found in flat namespace '_npy_copysign'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfTransformer \n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/__init__.py:82\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _distributor_init  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __check_build  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions\n\u001b[1;32m     85\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalibration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshow_versions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    129\u001b[0m ]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/base.py:17\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _IS_32BIT\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_tags\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     19\u001b[0m     _DEFAULT_TAGS,\n\u001b[1;32m     20\u001b[0m     _safe_tags,\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_X_y\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/__init__.py:28\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataConversionWarning\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecated\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfixes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m np_version, parse_version\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m estimator_html_repr\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     31\u001b[0m     as_float_array,\n\u001b[1;32m     32\u001b[0m     assert_all_finite,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m     check_scalar,\n\u001b[1;32m     41\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/fixes.py:20\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msp\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lsqr \u001b[38;5;28;01mas\u001b[39;00m sparse_lsqr  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mthreadpoolctl\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scipy/stats/__init__.py:441\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m.. _statsrefmanual:\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    438\u001b[0m \n\u001b[1;32m    439\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 441\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmorestats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scipy/stats/stats.py:37\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array, asarray, ma\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspatial\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistance\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cdist\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mndimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m measurements\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_util\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (check_random_state, MapWrapper,\n\u001b[1;32m     40\u001b[0m                               rng_integers, float_factorial)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scipy/spatial/__init__.py:102\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_plotutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_procrustes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m procrustes\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_geometric_slerp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m geometric_slerp\n\u001b[1;32m    104\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [s \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m    105\u001b[0m __all__ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistance\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scipy/spatial/_geometric_slerp.py:8\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspatial\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistance\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m euclidean\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_geometric_slerp\u001b[39m(start, end, t):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# create an orthogonal basis using QR decomposition\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     basis \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack([start, end])\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scipy/spatial/distance.py:121\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_util\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _asarray_validated\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _deprecated\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _distance_wrap\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _hausdorff\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m norm\n",
      "\u001b[0;31mImportError\u001b[0m: dlopen(/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scipy/spatial/_distance_wrap.cpython-310-darwin.so, 0x0002): symbol not found in flat namespace '_npy_copysign'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093478df",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words = text.ENGLISH_STOP_WORDS.union(science_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5dd797ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(text):\n",
    "    \"\"\"\n",
    "      A simple function to clean up the data. All the words that\n",
    "      are not designated as a stop word is then lemmatized after\n",
    "      encoding and basic regex parsing are performed.\n",
    "    \"\"\"\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    porter_stemmer=PorterStemmer()\n",
    "    stopwords =list(my_stop_words)\n",
    "#     print(stopwords)\n",
    "    text = (unicodedata.normalize('NFKD', text)\n",
    "    .encode('ascii', 'ignore')\n",
    "    .decode('utf-8', 'ignore')\n",
    "    .lower())\n",
    "     # remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    words = re.sub(r'[^\\w\\s]', '', text).split()\n",
    "    final_words = [wnl.lemmatize(word) for word in words if word not in stopwords]\n",
    "#     final_words=[porter_stemmer.stem(word=word) for word in final_words]\n",
    "    text =' '.join(final_words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "91301170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tokenizer(text):\n",
    "    # create a space between special characters \n",
    "    print(\"tokenize\",text)\n",
    "    text=re.sub(\"(\\\\W)\",\" \\\\1 \",text)\n",
    "\n",
    "    # split based on whitespace\n",
    "    return re.split(\"\\\\s+\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0cf27613",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# init stemmer\n",
    "porter_stemmer=PorterStemmer()\n",
    "\n",
    "def my_cool_preprocessor(text):\n",
    "    \n",
    "    text=text.lower() \n",
    "    text=re.sub(\"\\\\W\",\" \",text) # remove special chars\n",
    "#     text=re.sub(\"\\\\s+(in|the|all|for|and|on)\\\\s+\",\" _connector_ \",text) # normalize certain words\n",
    "    \n",
    "    # stem words\n",
    "    words=re.split(\"\\\\s+\",text)\n",
    "    stemmed_words=[porter_stemmer.stem(word=word) for word in words]\n",
    "    return ' '.join(stemmed_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "942fdd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_clean_nos(essay):\n",
    "    nonPunctEssay = re.sub(\"[^a-zA-Z\\\\s+,.]\", \" \", essay)\n",
    "    nonPunctEssay = re.sub(\"[^a-zA-Z]\", \" \", nonPunctEssay)\n",
    "    return nonPunctEssay\n",
    "def reg_remove_puntuation(each_essay):\n",
    "    each_essay = unicodedata.normalize('NFKD', each_essay).encode('ascii','ignore')\n",
    "    each_essay = each_essay.decode('utf-8')\n",
    "    each_essay= each_essay.replace(\"’\", \"'\")\n",
    "    each_essay= each_essay.replace(\"…\", \" \")\n",
    "    each_essay= each_essay.replace(\".\", \" \")\n",
    "    \n",
    "    each_essay = re.sub('\\[*?\\]\\\"\\'', '', each_essay)\n",
    "    each_essay = re.sub(\"[,?\\[\\]\\{\\}\\(\\):;<>\\\\\\/\\-_]\", \" \", each_essay)\n",
    "    nonPunctEssay = re.sub(\"[,?\\[\\]\\{\\}\\(\\)\\\":;<>]\", \" \", each_essay)\n",
    "    each_essay=each_essay.replace(\"@\", \" at \")\n",
    "    each_essay=each_essay.replace(\"#\", \" hash \")\n",
    "    each_essay=each_essay.replace(\"$\", \" dollar \")\n",
    "    each_essay=each_essay.replace(\"%\", \" percent \")\n",
    "    each_essay=each_essay.replace(\"'l \", \"l \")\n",
    "    nonPunctEssay = re.sub(\"[,?\\[\\]\\{\\}\\(\\)\\\"\\':;<>\\\\\\/\\-_]\", \"\", each_essay)\n",
    "    \n",
    "    return nonPunctEssay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ec3142f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols =['Authors','Title','Source_Title','Year','Link','Cited','Abstract','AuthorKey','Eng_Controlled','Eng_Unctrolled','Eng_MainHeading','Scival_topic','Scival_Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4b167045",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvpr = pd.read_csv('../Data/Paper_Details/PaperDetails_Gr2021_CVPR.csv',header=None)\n",
    "nips =pd.read_csv('../Data/Paper_Details/PaperDetails_Gr2021_NIPS.csv',header=None)\n",
    "kdd =pd.read_csv('../Data/Paper_Details/PaperDetails_Gr2021_KDD.csv',header=None)\n",
    "acm =pd.read_csv('../Data/Paper_Details/PaperDetails_Gr2021_ACM.csv',header=None)\n",
    "ieee =pd.read_csv('../Data/Paper_Details/PaperDetails_Gr2021_IEEE.csv',header=None)\n",
    "ind =pd.read_csv('../Data/Paper_Details/PaperDetails_Gr2021_IND.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a429410f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvpr.columns =cols\n",
    "nips.columns =cols\n",
    "kdd.columns =cols\n",
    "acm.columns =cols\n",
    "ieee.columns =cols\n",
    "ind.columns =cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8e0b1d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame()\n",
    "for df in [cvpr,nips,kdd,acm,ieee,ind]:\n",
    "    final_df = final_df.append(df)\n",
    "final_df.reset_index(drop=True,inplace=True)\n",
    "final_df.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ca628129",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_gap = 5\n",
    "start_year = 2000\n",
    "end_year = 2022\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c72dc68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_cols = ['Abstract','AuthorKey','Eng_Controlled','Eng_Unctrolled','Eng_MainHeading','Scival_topic']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ff8914",
   "metadata": {},
   "source": [
    "##  COunt Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "80dbd5bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year range ['2000-2001', '2001-2002', '2002-2003', '2003-2004', '2004-2005', '2005-2006', '2006-2007', '2007-2008', '2008-2009', '2009-2010', '2010-2011', '2011-2012', '2012-2013', '2013-2014', '2014-2015', '2015-2016', '2016-2017', '2017-2018', '2018-2019', '2019-2020', '2020-2021', '2021-2022']\n",
      "1 (1, 1)\n",
      "length of df 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11114/2026202.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  working_df['text'] =''\n",
      "/tmp/ipykernel_11114/2026202.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  working_df['text'] = working_df['text'] + working_df[col]\n",
      "/tmp/ipykernel_11114/2026202.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  working_df['text'] = working_df['text'].apply(lambda x:reg_remove_puntuation(x))\n",
      "/tmp/ipykernel_11114/2026202.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  working_df['text'] = working_df['text'].apply(lambda x:basic_clean(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 2001\n",
      "2001 2002\n",
      "2002 2003\n",
      "2003 2004\n",
      "2004 2005\n",
      "2005 2006\n",
      "2006 2007\n",
      "2007 2008\n",
      "2008 2009\n",
      "2009 2010\n",
      "2010 2011\n",
      "2011 2012\n",
      "2012 2013\n",
      "2013 2014\n",
      "2014 2015\n",
      "2015 2016\n",
      "2016 2017\n",
      "2017 2018\n",
      "2018 2019\n",
      "2019 2020\n",
      "2020 2021\n",
      "2021 2022\n",
      "file_write completed...\n",
      "1 (2, 2)\n",
      "length of df 0\n",
      "2000 2001\n",
      "2001 2002\n",
      "2002 2003\n",
      "2003 2004\n",
      "2004 2005\n",
      "2005 2006\n",
      "2006 2007\n",
      "2007 2008\n",
      "2008 2009\n",
      "2009 2010\n",
      "2010 2011\n",
      "2011 2012\n",
      "2012 2013\n",
      "2013 2014\n",
      "2014 2015\n",
      "2015 2016\n",
      "2016 2017\n",
      "2017 2018\n",
      "2018 2019\n",
      "2019 2020\n",
      "2020 2021\n",
      "2021 2022\n",
      "file_write completed...\n",
      "1 (3, 3)\n",
      "length of df 0\n",
      "2000 2001\n",
      "2001 2002\n",
      "2002 2003\n",
      "2003 2004\n",
      "2004 2005\n",
      "2005 2006\n",
      "2006 2007\n",
      "2007 2008\n",
      "2008 2009\n",
      "2009 2010\n",
      "2010 2011\n",
      "2011 2012\n",
      "2012 2013\n",
      "2013 2014\n",
      "2014 2015\n",
      "2015 2016\n",
      "2016 2017\n",
      "2017 2018\n",
      "2018 2019\n",
      "2019 2020\n",
      "2020 2021\n",
      "2021 2022\n",
      "file_write completed...\n",
      "year range ['2000-2002', '2002-2004', '2004-2006', '2006-2008', '2008-2010', '2010-2012', '2012-2014', '2014-2016', '2016-2018', '2018-2020', '2020-2022']\n",
      "2 (1, 1)\n",
      "length of df 0\n",
      "2000 2002\n",
      "2002 2004\n",
      "2004 2006\n",
      "2006 2008\n",
      "2008 2010\n",
      "2010 2012\n",
      "2012 2014\n",
      "2014 2016\n",
      "2016 2018\n",
      "2018 2020\n",
      "2020 2022\n",
      "file_write completed...\n",
      "2 (2, 2)\n",
      "length of df 0\n",
      "2000 2002\n",
      "2002 2004\n",
      "2004 2006\n",
      "2006 2008\n",
      "2008 2010\n",
      "2010 2012\n",
      "2012 2014\n",
      "2014 2016\n",
      "2016 2018\n",
      "2018 2020\n",
      "2020 2022\n",
      "file_write completed...\n",
      "2 (3, 3)\n",
      "length of df 0\n",
      "2000 2002\n",
      "2002 2004\n",
      "2004 2006\n",
      "2006 2008\n",
      "2008 2010\n",
      "2010 2012\n",
      "2012 2014\n",
      "2014 2016\n",
      "2016 2018\n",
      "2018 2020\n",
      "2020 2022\n",
      "file_write completed...\n",
      "year range ['2000-2003', '2003-2006', '2006-2009', '2009-2012', '2012-2015', '2015-2018', '2018-2021', '2021-2024']\n",
      "3 (1, 1)\n",
      "length of df 0\n",
      "2000 2003\n",
      "2003 2006\n",
      "2006 2009\n",
      "2009 2012\n",
      "2012 2015\n",
      "2015 2018\n",
      "2018 2021\n",
      "2021 2024\n",
      "file_write completed...\n",
      "3 (2, 2)\n",
      "length of df 0\n",
      "2000 2003\n",
      "2003 2006\n",
      "2006 2009\n",
      "2009 2012\n",
      "2012 2015\n",
      "2015 2018\n",
      "2018 2021\n",
      "2021 2024\n",
      "file_write completed...\n",
      "3 (3, 3)\n",
      "length of df 0\n",
      "2000 2003\n",
      "2003 2006\n",
      "2006 2009\n",
      "2009 2012\n",
      "2012 2015\n",
      "2015 2018\n",
      "2018 2021\n",
      "2021 2024\n",
      "file_write completed...\n",
      "year range ['2000-2005', '2005-2010', '2010-2015', '2015-2022']\n",
      "5 (1, 1)\n",
      "length of df 0\n",
      "2000 2005\n",
      "2005 2010\n",
      "2010 2015\n",
      "2015 2022\n",
      "file_write completed...\n",
      "5 (2, 2)\n",
      "length of df 0\n",
      "2000 2005\n",
      "2005 2010\n",
      "2010 2015\n",
      "2015 2022\n",
      "file_write completed...\n",
      "5 (3, 3)\n",
      "length of df 0\n",
      "2000 2005\n",
      "2005 2010\n",
      "2010 2015\n",
      "2015 2022\n",
      "file_write completed...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bigram_num = 50\n",
    "year_gaps = [1,2,3,5]\n",
    "ngrams = {'uni':(1,1),'bigram':(2,2),'trigram':(3,3)}\n",
    "\n",
    "for year_gap in year_gaps:\n",
    "    names = []\n",
    "    year = start_year\n",
    "    while( year<end_year):\n",
    "        first = year\n",
    "        last = year+year_gap\n",
    "        if last==2020 and year_gap==5:\n",
    "            last=2022\n",
    "        str_ = str(first)+'-'+str(last)\n",
    "        names.append(str_)\n",
    "        year = last\n",
    "    print(\"year range\",names)\n",
    "    \n",
    "    \n",
    "    for txt,ngramR in ngrams.items():\n",
    "        \n",
    "        print(year_gap,ngramR)\n",
    "        df_list = []\n",
    "        print(\"length of df\",len(df_list))\n",
    "        year = start_year\n",
    "        for yr_str in names:\n",
    "            first,last = yr_str.split('-')\n",
    "        \n",
    "#         while(year<end_year):\n",
    "#             first = year\n",
    "#             last = year+year_gap\n",
    "#             if last==2020 and year_gap==5:\n",
    "#                 last=2021\n",
    "            working_df = final_df[(final_df['Year']>=int(first))&(final_df['Year']<int(last))]\n",
    "            working_df['text'] =''\n",
    "            for col in working_cols:\n",
    "                working_df['text'] = working_df['text'] + working_df[col]\n",
    "            working_df['text'] = working_df['text'].apply(lambda x:reg_remove_puntuation(x))\n",
    "            working_df['text'] = working_df['text'].apply(lambda x:basic_clean(x))\n",
    "            docs=working_df['text'].tolist()\n",
    "            docs = [i for i in docs if i!=' ']\n",
    "            cv = CountVectorizer(ngram_range=ngramR,stop_words=list(my_stop_words),max_features=2000)#,tokenizer=my_tokenizer,preprocessor=basic_clean)\n",
    "            count_vector=cv.fit_transform(docs)\n",
    "        #     word_set = basic_clean(''.join(str(working_df['text'].tolist())))\n",
    "        #     print('initial_wordset',word_set[:20])\n",
    "            feature_array = np.array(cv.get_feature_names())\n",
    "            cv_sorting = np.argsort(count_vector.toarray()).flatten()[::-1]\n",
    "            print(first,last)\n",
    "            n = bigram_num\n",
    "            top_n = feature_array[cv_sorting][:n]\n",
    "#             print(top_n)\n",
    "            x = {k: v for k, v in cv.vocabulary_.items() if k in top_n}\n",
    "            xdf =pd.DataFrame(x.items())\n",
    "            xdf.sort_values(by=[1],ascending=False)\n",
    "            df_list.append(xdf)\n",
    "            year = last\n",
    "        writer=pd.ExcelWriter(r\"../Data/Paper_Details/Results/\"+txt+\"_CountVec_gap_\"+str(year_gap)+\".xlsx\")\n",
    "        _ = [A.to_excel(writer,sheet_name=\"{0}\".format(names[i])) for i, A in enumerate(df_list)]\n",
    "        print(\"file_write completed...\")\n",
    "        writer.save()\n",
    "        #     print(first,last)\n",
    "        #     bigram_size = (pd.Series(nltk.ngrams(word_set, 3)).value_counts())[:bigram_num]\n",
    "        #     df_list.append(pd.DataFrame(bigram_size))\n",
    "        #     bigram_size.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))\n",
    "        #     year = last\n",
    "        #     plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "51c36dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict(xdf.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cde1b8",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "822758a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(text):\n",
    "    \n",
    "    # lowercase\n",
    "    text=text.lower()\n",
    "    \n",
    "    #remove tags\n",
    "    text=re.sub(\"\",\"\",text)\n",
    "    \n",
    "    # remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "40f3e632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year range ['2000-2001', '2001-2002', '2002-2003', '2003-2004', '2004-2005', '2005-2006', '2006-2007', '2007-2008', '2008-2009', '2009-2010', '2010-2011', '2011-2012', '2012-2013', '2013-2014', '2014-2015', '2015-2016', '2016-2017', '2017-2018', '2018-2019', '2019-2020', '2020-2021', '2021-2022']\n",
      "1 (1, 1) []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11114/2420360605.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  working_df['text'] =''\n",
      "/tmp/ipykernel_11114/2420360605.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  working_df['text'] = working_df['text'] + working_df[col]\n",
      "/tmp/ipykernel_11114/2420360605.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  working_df['text'] = working_df['text'].apply(lambda x:reg_remove_puntuation(x))\n",
      "/tmp/ipykernel_11114/2420360605.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  working_df['text'] = working_df['text'].apply(lambda x:basic_clean(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 2001\n",
      "2001 2002\n",
      "2002 2003\n",
      "2003 2004\n",
      "2004 2005\n",
      "2005 2006\n",
      "2006 2007\n",
      "2007 2008\n",
      "2008 2009\n",
      "2009 2010\n",
      "2010 2011\n",
      "2011 2012\n",
      "2012 2013\n",
      "2013 2014\n",
      "2014 2015\n",
      "2015 2016\n",
      "2016 2017\n",
      "2017 2018\n",
      "2018 2019\n",
      "2019 2020\n",
      "2020 2021\n",
      "2021 2022\n",
      "file write completed\n",
      "1 (2, 2) []\n",
      "2000 2001\n",
      "2001 2002\n",
      "2002 2003\n",
      "2003 2004\n",
      "2004 2005\n",
      "2005 2006\n",
      "2006 2007\n",
      "2007 2008\n",
      "2008 2009\n",
      "2009 2010\n",
      "2010 2011\n",
      "2011 2012\n",
      "2012 2013\n",
      "2013 2014\n",
      "2014 2015\n",
      "2015 2016\n",
      "2016 2017\n",
      "2017 2018\n",
      "2018 2019\n",
      "2019 2020\n",
      "2020 2021\n",
      "2021 2022\n",
      "file write completed\n",
      "1 (3, 3) []\n",
      "2000 2001\n",
      "2001 2002\n",
      "2002 2003\n",
      "2003 2004\n",
      "2004 2005\n",
      "2005 2006\n",
      "2006 2007\n",
      "2007 2008\n",
      "2008 2009\n",
      "2009 2010\n",
      "2010 2011\n",
      "2011 2012\n",
      "2012 2013\n",
      "2013 2014\n",
      "2014 2015\n",
      "2015 2016\n",
      "2016 2017\n",
      "2017 2018\n",
      "2018 2019\n",
      "2019 2020\n",
      "2020 2021\n",
      "2021 2022\n",
      "file write completed\n",
      "year range ['2000-2002', '2002-2004', '2004-2006', '2006-2008', '2008-2010', '2010-2012', '2012-2014', '2014-2016', '2016-2018', '2018-2020', '2020-2022']\n",
      "2 (1, 1) []\n",
      "2000 2002\n",
      "2002 2004\n",
      "2004 2006\n",
      "2006 2008\n",
      "2008 2010\n",
      "2010 2012\n",
      "2012 2014\n",
      "2014 2016\n",
      "2016 2018\n",
      "2018 2020\n",
      "2020 2022\n",
      "file write completed\n",
      "2 (2, 2) []\n",
      "2000 2002\n",
      "2002 2004\n",
      "2004 2006\n",
      "2006 2008\n",
      "2008 2010\n",
      "2010 2012\n",
      "2012 2014\n",
      "2014 2016\n",
      "2016 2018\n",
      "2018 2020\n",
      "2020 2022\n",
      "file write completed\n",
      "2 (3, 3) []\n",
      "2000 2002\n",
      "2002 2004\n",
      "2004 2006\n",
      "2006 2008\n",
      "2008 2010\n",
      "2010 2012\n",
      "2012 2014\n",
      "2014 2016\n",
      "2016 2018\n",
      "2018 2020\n",
      "2020 2022\n",
      "file write completed\n",
      "year range ['2000-2003', '2003-2006', '2006-2009', '2009-2012', '2012-2015', '2015-2018', '2018-2021', '2021-2024']\n",
      "3 (1, 1) []\n",
      "2000 2003\n",
      "2003 2006\n",
      "2006 2009\n",
      "2009 2012\n",
      "2012 2015\n",
      "2015 2018\n",
      "2018 2021\n",
      "2021 2024\n",
      "file write completed\n",
      "3 (2, 2) []\n",
      "2000 2003\n",
      "2003 2006\n",
      "2006 2009\n",
      "2009 2012\n",
      "2012 2015\n",
      "2015 2018\n",
      "2018 2021\n",
      "2021 2024\n",
      "file write completed\n",
      "3 (3, 3) []\n",
      "2000 2003\n",
      "2003 2006\n",
      "2006 2009\n",
      "2009 2012\n",
      "2012 2015\n",
      "2015 2018\n",
      "2018 2021\n",
      "2021 2024\n",
      "file write completed\n",
      "year range ['2000-2005', '2005-2010', '2010-2015', '2015-2022']\n",
      "5 (1, 1) []\n",
      "2000 2005\n",
      "2005 2010\n",
      "2010 2015\n",
      "2015 2022\n",
      "file write completed\n",
      "5 (2, 2) []\n",
      "2000 2005\n",
      "2005 2010\n",
      "2010 2015\n",
      "2015 2022\n",
      "file write completed\n",
      "5 (3, 3) []\n",
      "2000 2005\n",
      "2005 2010\n",
      "2010 2015\n",
      "2015 2022\n",
      "file write completed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bigram_num = 50\n",
    "year_gaps = [1,2,3,5]\n",
    "ngrams = {'uni':(1,1),'bigram':(2,2),'trigram':(3,3)}\n",
    "\n",
    "for year_gap in year_gaps:\n",
    "    names = []\n",
    "    year = start_year\n",
    "    while( year<end_year):\n",
    "        first = year\n",
    "        last = year+year_gap\n",
    "        if last==2020 and year_gap==5:\n",
    "            last=2022\n",
    "        str_ = str(first)+'-'+str(last)\n",
    "        names.append(str_)\n",
    "        year = last\n",
    "        \n",
    "    print(\"year range\",names)\n",
    "    for txt,ngramR in ngrams.items():\n",
    "        df_tf_list = []\n",
    "        print(year_gap,ngramR,df_tf_list)\n",
    "        for yr_str in names:\n",
    "            first,last = yr_str.split('-')\n",
    "            working_df = final_df[(final_df['Year']>=int(first))&(final_df['Year']<int(last))]\n",
    "            working_df['text'] =''\n",
    "            for col in working_cols:\n",
    "                working_df['text'] = working_df['text'] + working_df[col]\n",
    "            working_df['text'] = working_df['text'].apply(lambda x:reg_remove_puntuation(x))\n",
    "            working_df['text'] = working_df['text'].apply(lambda x:basic_clean(x))\n",
    "            docs=working_df['text'].tolist()\n",
    "            tfidf_vectorizer=TfidfVectorizer(use_idf=True,smooth_idf=True,ngram_range=ngramR,max_features=2000,stop_words=my_stop_words) \n",
    "            tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(docs)\n",
    "            feature_array = np.array(tfidf_vectorizer.get_feature_names())\n",
    "            tfidf_sorting = np.argsort(tfidf_vectorizer_vectors.toarray()).flatten()[::-1]\n",
    "            print(first,last)\n",
    "            n = bigram_num\n",
    "            top_n = feature_array[tfidf_sorting][:n]\n",
    "#             df_tf_list.append(pd.DataFrame(top_n))\n",
    "#             print(\"Top N Keywords\\n\\n\")\n",
    "#             print(top_n)\n",
    "            year = last\n",
    "            x = {k: v for k, v in tfidf_vectorizer.vocabulary_.items() if k in top_n}\n",
    "            xdf =pd.DataFrame(x.items())\n",
    "#             print(xdf)\n",
    "            xdf.sort_values(by=[1],ascending=False)\n",
    "            df_tf_list.append(xdf)\n",
    "        writer=pd.ExcelWriter(r\"../Data/Paper_Details/Results/\"+txt+\"_TFiDF_gap_\"+str(year_gap)+\".xlsx\")\n",
    "        _ = [A.to_excel(writer,sheet_name=\"{0}\".format(names[i])) for i, A in enumerate(df_tf_list)]\n",
    "        writer.save()\n",
    "        print(\"file write completed\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc764a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writer=pd.ExcelWriter(r\"../Data/Paper_Details/Results/Trigrams_TFIDF.xlsx\")\n",
    "# _ = [A.to_excel(writer,sheet_name=\"{0}\".format(names[i])) for i, A in enumerate(df_tf_list)]\n",
    "# writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0720f3b7",
   "metadata": {},
   "source": [
    "## Streamlit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0dab2f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "import streamlit as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "69a21042",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip freeze > ../Scripts/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817a9041",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
